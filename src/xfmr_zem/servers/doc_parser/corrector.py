"""
File: /app/xfmr-zem/src/xfmr_zem/servers/doc_parser/corrector.py
Description: Vietnamese Legal Text Corrector.
"""

import sys
import os
import re
import torch
from typing import Any, Dict, Optional, List, Union
from loguru import logger
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm

# Import Zem
from xfmr_zem.server import ZemServer

# Setup logging
logger.remove()
logger.add(sys.stderr, level="INFO")

server = ZemServer("corrector", parameter_file=os.path.join(os.path.dirname(__file__), "parameters.yml"))

# Global cache for model to avoid reloading
_model_cache = {"model": None, "tokenizer": None, "device": None}

@server.tool()
def protonx_legal_markdown_corrector(
    data: Any,
    field: str = "markdown",  # Tr∆∞·ªùng input c·∫ßn l·∫•y d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω
    model_path: Optional[str] = None,
    tokenizer_config: Dict[str, Any] = None,
    seq2seq_config: Dict[str, Any] = None,
) -> Any:
    """
    Nh·∫≠n v√†o items, l·∫•y text t·ª´ `field`, x·ª≠ l√Ω v√† tr·∫£ v·ªÅ list c√°c dict {"markdown": "k·∫øt qu·∫£"}.
    """

    items = server.get_data(data)
    items = items if isinstance(items, list) else [items]

    # =========================================================================
    # PH·∫¶N 1: LOAD MODEL
    # =========================================================================
    
    tokenizer_params = server.parameters['protonx_legal_text_corrector']['tokenizer_config']
    seq2seq_params = server.parameters['protonx_legal_text_corrector']['seq2seq_config']
    model_path_param = server.parameters['protonx_legal_text_corrector']['model_path']

    if _model_cache["model"] is None:
        if model_path_param:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"üöÄ Loading VietnameseCorrector model from: {model_path_param} ({device})")
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path_param)
                model = AutoModelForSeq2SeqLM.from_pretrained(model_path_param).to(device)
                
                _model_cache["tokenizer"] = tokenizer
                _model_cache["model"] = model
                _model_cache["device"] = device
            except Exception as e:
                logger.error(f"Failed to load model: {e}")
                # N·∫øu l·ªói load model, tr·∫£ v·ªÅ danh s√°ch r·ªóng ho·∫∑c l·ªói t∆∞∆°ng ·ª©ng
                return server.save_output([{"markdown": ""} for _ in items])
        else:
            logger.warning("No model_path found in parameters.")

    # =========================================================================
    # PH·∫¶N 2: Inner Functions
    # =========================================================================

    def correct_text(text: str) -> str:
        """H√†m n·ªôi b·ªô: Ch·∫°y model AI ƒë·ªÉ s·ª≠a text thu·∫ßn"""
        if _model_cache["model"] is None or not text or not isinstance(text, str) or not text.strip():
            return text

        tokenizer = _model_cache["tokenizer"]
        model = _model_cache["model"]
        device = _model_cache["device"]

        inputs = tokenizer(
            text,
            truncation=tokenizer_params.get('truncation', True),
            max_length=tokenizer_params.get('max_length', 512),
            return_tensors='pt'
        ).to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                num_beams=seq2seq_params.get('num_beams', 5),
                max_new_tokens=seq2seq_params.get('max_new_tokens', 512),
                early_stopping=seq2seq_params.get('early_stopping', True)
            )

        return tokenizer.decode(outputs[0], skip_special_tokens=True)

    def process_markdown_text(markdown_content: str) -> str:
        """H√†m n·ªôi b·ªô: Parse v√† t√°i t·∫°o Markdown"""
        if not markdown_content or not isinstance(markdown_content, str):
            return ""

        # Regex patterns definition
        inline_code_pattern = re.compile(r'(`[^`]+`)')
        link_pattern = re.compile(r'(\[[^\]]+\]\([^)]+\))')
        image_pattern = re.compile(r'(!\[[^\]]*\]\([^)]+\))')
        html_tag_pattern = re.compile(r'(<[^>]+>)')
        url_pattern = re.compile(r'(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)')

        def _process_segment(text):
            if not text or not text.strip(): return text
            protected_segments = []
            def protect_match(match):
                protected_segments.append(match.group(0))
                return f"__PROTECTED_{len(protected_segments)-1}__"

            temp_text = inline_code_pattern.sub(protect_match, text)
            temp_text = image_pattern.sub(protect_match, temp_text)
            temp_text = link_pattern.sub(protect_match, temp_text)
            temp_text = url_pattern.sub(protect_match, temp_text)
            temp_text = html_tag_pattern.sub(protect_match, temp_text)

            if temp_text.strip() and not re.match(r'^__PROTECTED_\d+__$', temp_text.strip()):
                 corrected_text = correct_text(temp_text)
            else:
                 corrected_text = temp_text

            for i, segment in enumerate(protected_segments):
                corrected_text = corrected_text.replace(f"__PROTECTED_{i}__", segment)
            return corrected_text

        lines = markdown_content.split('\n')
        result_lines = []
        in_code_block = False
        
        line_patterns = [
            re.compile(r'^(#{1,6}\s+)(.*)'), 
            re.compile(r'^(\s*>\s+)(.*)'), 
            re.compile(r'^(\s*(?:[-*+]|\d+\.)\s+)(.*)')
        ]

        for line in tqdm(lines, desc = "Correcting lines Progress"):
            if line.strip().startswith('```'):
                in_code_block = not in_code_block
                result_lines.append(line)
                continue
            if in_code_block or not line.strip() or re.match(r'^[-*_]{3,}$', line.strip()):
                result_lines.append(line)
                continue

            matched = False
            for pattern in line_patterns:
                match = pattern.match(line)
                if match:
                    prefix, content = match.groups()
                    result_lines.append(f"{prefix}{_process_segment(content)}")
                    matched = True
                    break
            if matched: continue

            if '|' in line and re.match(r'^\s*\|.*\|\s*$', line):
                parts = line.split('|')
                res_p = [p if re.match(r'^\s*:?-+:?\s*$', p) else _process_segment(p) for p in parts]
                result_lines.append('|'.join(res_p))
                continue

            result_lines.append(_process_segment(line))
        return '\n'.join(result_lines)

    # =========================================================================
    # PH·∫¶N 3: Main Executor (Updated logic)
    # =========================================================================
    
    all_results = []
    
    for item in tqdm(items, desc="Processing Items"):
        try:
            # Logic: C·ªë g·∫Øng l·∫•y d·ªØ li·ªáu t·ª´ field. 
            # N·∫øu item kh√¥ng ph·∫£i dict ho·∫∑c kh√¥ng c√≥ key -> nh·∫£y v√†o except
            raw_text = item[field]
            
            # X·ª≠ l√Ω vƒÉn b·∫£n
            corrected_text = process_markdown_text(raw_text)
            
            # Output ra list m·ªõi v·ªõi key c·ªë ƒë·ªãnh l√† "markdown"
            all_results.append({"markdown": corrected_text})
            
        except Exception as e:
            # Log l·ªói (v√≠ d·ª•: KeyError n·∫øu kh√¥ng t√¨m th·∫•y field)
            logger.warning(f"Error extracting/processing field '{field}': {e}")
            
            # N·∫øu l·ªói, tr·∫£ v·ªÅ chu·ªói r·ªóng cho key markdown
            all_results.append({"markdown": ""})

    return server.save_output(all_results)

if __name__ == "__main__":
    server.run()